{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from DLAIUtils import Utils\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "utils = Utils()\n",
    "PINECONE_API_KEY = utils.get_pinecone_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Pinecone\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "utils = Utils()\n",
    "INDEX_NAME = utils.create_dlai_index_name('dl-ai')\n",
    "\n",
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
    "  pinecone.delete_index(INDEX_NAME)\n",
    "pinecone.create_index(\n",
    "  INDEX_NAME,\n",
    "  dimension=512,\n",
    "  metric=\"dotproduct\",\n",
    "  spec=ServerlessSpec(cloud='aws', region='us-west-2')\n",
    ")\n",
    "index = pinecone.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "fashion = load_dataset(\n",
    "    \"ashraq/fashion-product-images-small\",\n",
    "    split=\"train\"\n",
    ")\n",
    "fashion\n",
    "\n",
    "images = fashion['image']\n",
    "metadata = fashion.remove_columns('image')\n",
    "images[900]\n",
    "\n",
    "metadata = metadata.to_pandas()\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Sparse Vector Using BM32\n",
    "bm25 = BM25Encoder()\n",
    "bm25.fit(metadata['productDisplayName'])\n",
    "metadata['productDisplayName'][0]\n",
    "\n",
    "bm25.encode_queries(metadata['productDisplayName'][0])\n",
    "bm25.encode_documents(metadata['productDisplayName'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Dense vector using CLIP\n",
    "model = SentenceTransformer('sentence-transformers/clip-ViT-B-32', device = device)\n",
    "model\n",
    "dense_vec = model.encode([metadata['productDisplayName'][0]])\n",
    "dense_vec.shape\n",
    "len(fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embeddings Using Sparse and Dense\n",
    "batch_size = 100\n",
    "fashion_data_num = 1000 # This is artificially low for speedier reults\n",
    "for i in tqdm(range(0, fashion_data_num, batch_size)):\n",
    "    i_end = min(i + batch_size, len(fashion))\n",
    "    meta_batch = metadata.iloc[i:i_end]\n",
    "    meta_dict = meta_batch.to_dict(orient='records')\n",
    "\n",
    "    # Concatenate all metadata fields except for id and year\n",
    "    meta_batch = [\" \".join(x) for x in meta_batch.loc[:, ~meta_batch.columns.isin(['id', 'year'])].values.to_list()]\n",
    "    img_batch = images[i:i_end]\n",
    "    sparse_embeds = bm25.encode_documents([text for text in meta_batch])\n",
    "    dense_emdeds = model.encode(img_batch).tolist()\n",
    "    ids = [str(x) for x in range(i, i_end)]\n",
    "\n",
    "    upserts = []\n",
    "    for _id, sparse, dense, meta in zip(ids, sparse_embeds, dense_emdeds, meta_dict):\n",
    "        upserts.append({\n",
    "            \"id\": _id,\n",
    "            \"sparse_values\": sparse,\n",
    "            \"values\": dense,\n",
    "            \"metadata\": meta\n",
    "        })\n",
    "    index.upsert(upserts)\n",
    "\n",
    "index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your query\n",
    "query = \"dark blue french connection jeans for men\"\n",
    "sparse = bm25.encode_queries(query)\n",
    "dense = model.encode([query]).tolist()\n",
    "result = index.query(\n",
    "    top_k=14,\n",
    "    vector=dense,\n",
    "    sparse_vector=sparse\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "imgs = [images[int(r['id'])] for r in result['matches']]\n",
    "imgs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "from io import BytesIO\n",
    "from base64 import b64encode\n",
    "\n",
    "# function to display product images\n",
    "def display_result(image_batch):\n",
    "    figures = []\n",
    "    for img in image_batch:\n",
    "        b = BytesIO()\n",
    "        img.save(b, format='png')\n",
    "        figures.append(f'''\n",
    "            <figure style=\"margin: 5px !important;\">\n",
    "              <img src=\"data:image/png;base64,{b64encode(b.getvalue()).decode('utf-8')}\" style=\"width: 90px; height: 120px\" >\n",
    "            </figure>\n",
    "        ''')\n",
    "    return HTML(data=f'''\n",
    "        <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n",
    "        {''.join(figures)}\n",
    "        </div>\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the Hybrid Search\n",
    "def hybrid_scale(dense, sparse, alpha: float):\n",
    "    \"\"\"Hybrid vector scaling using a convex combination\n",
    "\n",
    "    alpha * dense + (1 - alpha) * sparse\n",
    "\n",
    "    Args:\n",
    "        dense: Array of floats representing\n",
    "        sparse: a dict of `indices` and `values`\n",
    "        alpha: float between 0 and 1 where 0 == sparse only\n",
    "               and 1 == dense only\n",
    "    \"\"\"\n",
    "    if alpha < 0 or alpha > 1:\n",
    "        raise ValueError(\"Alpha must be between 0 and 1\")\n",
    "    hsparse = {\n",
    "        \"indices\": sparse[\"indices\"],\n",
    "        'values': [V * (1 - alpha) for v in sparse['values']]\n",
    "    }\n",
    "    hdense = [v * alpha for v in dense]\n",
    "    return hdense, hsparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Dense\n",
    "question = \"dark blue french connection jeans for men\"\n",
    "hdense, hsparse = hybrid_scale(dense, sparse, alpha=1)\n",
    "result = index.query(\n",
    "    top_k=6,\n",
    "    vector=hdense,\n",
    "    sparse_vector=hsparse,\n",
    "    include_metadata=True\n",
    ")\n",
    "imgs = [images[int(r['id'])] for r in result['matches']]\n",
    "display_result(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Sparse\n",
    "question = \"dark blue french connection jeans for men\"\n",
    "#Closer to 0==more sparse, closer to 1==more dense\n",
    "hdense, hsparse = hybrid_scale(dense, sparse, alpha=0)\n",
    "result = index.query(\n",
    "    top_k=6,\n",
    "    vector=hdense,\n",
    "    sparse_vector=hsparse,\n",
    "    include_metadata=True\n",
    ")\n",
    "imgs = [images[int(r[\"id\"])] for r in result[\"matches\"]]\n",
    "display_result(imgs)\n",
    "\n",
    "for x in result['matches']:\n",
    "    print(x['metadata']['productDisplayName'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
