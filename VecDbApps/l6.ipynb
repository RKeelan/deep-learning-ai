{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, util\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from DLAIUtils import Utils\n",
    "import torch\n",
    "import time\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Pinecone\n",
    "utils = Utils()\n",
    "PINECONE_API_KEY = utils.get_pinecone_api_key()\n",
    "\n",
    "INDEX_NAME = utils.create_dlai_index_name('dl-ai')\n",
    "\n",
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
    "  pinecone.delete_index(INDEX_NAME)\n",
    "pinecone.create_index(name=INDEX_NAME, dimension=256, metric='cosine',\n",
    "  spec=ServerlessSpec(cloud='aws', region='us-west-2'))\n",
    "index = pinecone.Index(INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "#!wget -q --show-progress -O training.tar.zip \"https://www.dropbox.com/scl/fi/rihfngx4ju5pzjzjj7u9z/lesson6.tar.zip?rlkey=rct9a9bo8euqgshrk8wiq2orh&dl=1\"\n",
    "#!tar -xzvf training.tar.zip\n",
    "#!tar -xzvf lesson6.tar\n",
    "!head -5 sample.log\n",
    "!head -5 training.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "word_embedding_model = models.Transformer('bert-base-uncased')\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model], device=device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "train_examples = []\n",
    "with open('./training.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            a, b, label = line.split('\\t')\n",
    "            train_examples.append(InputExample(texts=[a, b], label=float(label)))\n",
    "\n",
    "# Define dataset, the datloader, and the training loss\n",
    "warmup_steps = 100\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "load_pretrained_model = True # Set to False to train from scratch\n",
    "if load_pretrained_model:\n",
    "    trained_model_file = open('./data/pretrained', 'rb')\n",
    "    db = pickle.load\n",
    "    trained_model_file.close()\n",
    "else:\n",
    "    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=16, warmup_steps=warmup_steps)\n",
    "\n",
    "samples = []\n",
    "with open('./sample.log', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            #emb = model.encode([line])\n",
    "            samples.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.encode(samples)\n",
    "prepped = []\n",
    "for i in tqdm(range(len(samples))):\n",
    "    v = {'id':f'{i}', 'values':emb[i].tolist(), 'metadata':{'log':samples[i]}}\n",
    "    prepped.append(v)\n",
    "index.upsert(prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine the Anomaly\n",
    "good_log_line = samples[0]\n",
    "print(good_log_line)\n",
    "results = []\n",
    "while len(results) == 0:\n",
    "    time.sleep(2)\n",
    "    queried = index.query(\n",
    "        vector=emb[0].tolist(),\n",
    "        include_metadata=True,\n",
    "        top_k=100\n",
    "    )\n",
    "    results = queried['matches']\n",
    "    print(\".:. \", end='')\n",
    "\n",
    "for i in range(0,10):\n",
    "    print(f\"{round(results[i]['score'], 4)}\\t{results[i]['metadata']['log']}\")\n",
    "\n",
    "last_element = len(samples) - 1\n",
    "print(f\"{round(results[last_element]['score'], 4)}\\t{results[last_element]['metadata']['log']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
